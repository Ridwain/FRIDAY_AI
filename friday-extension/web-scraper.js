// web-scraper.js
// Utilities for URL Q&A, web scraping+indexing, and SERP proxy

const SERVER_URL = 'http://localhost:3000';

// ---- simple persisted cache of already-upserted URLs ----
const UPSERT_CACHE_KEY = 'webUrlUpserts';
let upsertedUrlSet = new Set();

async function loadUpsertCache() {
  try {
    if (!chrome?.storage?.local) return;
    const result = await chrome.storage.local.get(UPSERT_CACHE_KEY);
    const arr = result?.[UPSERT_CACHE_KEY] || [];
    upsertedUrlSet = new Set(arr);
  } catch (e) {
    // ignore; non-fatal
  }
}
async function saveUpsertCache() {
  try {
    if (!chrome?.storage?.local) return;
    await chrome.storage.local.set({ [UPSERT_CACHE_KEY]: [...upsertedUrlSet] });
  } catch (e) {
    // ignore; non-fatal
  }
}

// Call this once from chat.js during init
export async function initWebScraper() {
  await loadUpsertCache();
}

// ---- helpers ----
export function isUrlLike(str = '') {
  return /^https?:\/\/\S+/i.test(String(str).trim());
}

export function detectWebSearchyQuery(q = '') {
  const s = q.toLowerCase();
  return /\b(search|google|bing|duckduckgo|latest|news|trending|what's new|recent|top results)\b/.test(s);
}

function sleep(ms) { return new Promise(r => setTimeout(r, ms)); }

async function postJSON(path, body, timeoutMs = 25000) {
  const opts = {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(body),
    signal: AbortSignal.timeout ? AbortSignal.timeout(timeoutMs) : undefined
  };
  const r = await fetch(`${SERVER_URL}${path}`, opts);
  if (!r.ok) {
    let t = '';
    try { t = await r.text(); } catch {}
    throw new Error(`${path} failed: ${r.status} ${t}`);
  }
  return r.json();
}

// ---- server calls ----
export async function scrapeUrl(url) {
  return postJSON('/scrape-url', { url }, 20000); // {title,url,text}
}

export async function scrapeAndUpsert(url, namespaceHint) {
  if (upsertedUrlSet.has(url)) {
    return { cached: true, message: 'Already upserted (client cache)' };
  }
  const out = await postJSON('/scrape-and-upsert', { url, namespaceHint }, 45000);
  if (out?.success) {
    upsertedUrlSet.add(url);
    await saveUpsertCache();
  }
  return out;
}

export async function serpSearch(q) {
  const j = await postJSON('/serp/search', { q }, 15000);
  return Array.isArray(j.results) ? j.results : [];
}

// ---- Drive link scanning & bulk index ----
const URL_RE = /https?:\/\/[^\s"')\]]+/g;

export function extractUrlsFromText(text = '') {
  return (text.match(URL_RE) || [])
    .filter(u => !/https:\/\/(drive\.google\.com|meet\.google\.com)\//.test(u))
    .map(u => u.replace(/[),.;]+$/, '').trim());
}

export async function indexUrlsFromFiles(filesContentMap = {}, namespaceHint, throttleMs = 1400) {
  const found = new Set();
  for (const text of Object.values(filesContentMap || {})) {
    if (!text || typeof text !== 'string') continue;
    extractUrlsFromText(text).forEach(u => found.add(u));
  }
  const list = [...found];
  if (!list.length) return { scanned: 0, queued: 0 };

  let ok = 0, fail = 0;
  for (const u of list) {
    try {
      if (upsertedUrlSet.has(u)) continue;
      await scrapeAndUpsert(u, namespaceHint);
      ok++;
      await sleep(throttleMs);
    } catch (e) {
      fail++;
      await sleep(Math.min(throttleMs * 2, 4000));
    }
  }
  return { scanned: list.length, upserted: ok, failed: fail };
}

// ---- prompt builder for pageâ€‘only Q&A ----
export function buildMessagesForUrlQA(question, scrapedText) {
  const context = (scrapedText || '').slice(0, 12000);
  return [
    {
      role: 'system',
      content: `You are a precise assistant. Answer ONLY from the provided webpage content.
If the answer is not in the content, say you don't have enough info. Keep it concise.`
    },
    { role: 'system', content: `WEBPAGE CONTENT:\n${context}` },
    { role: 'user', content: question }
  ];
}
